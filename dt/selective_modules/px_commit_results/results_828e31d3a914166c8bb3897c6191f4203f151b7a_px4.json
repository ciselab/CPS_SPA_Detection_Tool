{"sha":"828e31d3a914166c8bb3897c6191f4203f151b7a","node_id":"MDY6Q29tbWl0NTI5ODc5MDo4MjhlMzFkM2E5MTQxNjZjOGJiMzg5N2M2MTkxZjQyMDNmMTUxYjdh","commit":{"author":{"name":"Beat KÃ¼ng","email":"beat-kueng@gmx.net","date":"2019-01-09T12:23:57Z"},"committer":{"name":"Julian Oes","email":"julian@oes.ch","date":"2019-01-14T10:08:48Z"},"message":"lockstep_scheduler: optimize performance\n\n- use a linked-list instead of std::vector. Insertion and removal are now\n  O(1)\n- avoid malloc and use a thread_local instance of TimedWait.\n  It gets destroyed when the thread exits, so we have to add protection\n  in case a thread exits too quickly. This in turn requires a fix to the\n  unit-tests.","tree":{"sha":"e0aaff1049c294a29c1ebeeede68a6bc5a172006","url":"https://api.github.com/repos/PX4/PX4-Autopilot/git/trees/e0aaff1049c294a29c1ebeeede68a6bc5a172006"},"url":"https://api.github.com/repos/PX4/PX4-Autopilot/git/commits/828e31d3a914166c8bb3897c6191f4203f151b7a","comment_count":0,"verification":{"verified":false,"reason":"unsigned","signature":null,"payload":null}},"url":"https://api.github.com/repos/PX4/PX4-Autopilot/commits/828e31d3a914166c8bb3897c6191f4203f151b7a","html_url":"https://github.com/PX4/PX4-Autopilot/commit/828e31d3a914166c8bb3897c6191f4203f151b7a","comments_url":"https://api.github.com/repos/PX4/PX4-Autopilot/commits/828e31d3a914166c8bb3897c6191f4203f151b7a/comments","author":{"login":"bkueng","id":281593,"node_id":"MDQ6VXNlcjI4MTU5Mw==","avatar_url":"https://avatars.githubusercontent.com/u/281593?v=4","gravatar_id":"","url":"https://api.github.com/users/bkueng","html_url":"https://github.com/bkueng","followers_url":"https://api.github.com/users/bkueng/followers","following_url":"https://api.github.com/users/bkueng/following{/other_user}","gists_url":"https://api.github.com/users/bkueng/gists{/gist_id}","starred_url":"https://api.github.com/users/bkueng/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bkueng/subscriptions","organizations_url":"https://api.github.com/users/bkueng/orgs","repos_url":"https://api.github.com/users/bkueng/repos","events_url":"https://api.github.com/users/bkueng/events{/privacy}","received_events_url":"https://api.github.com/users/bkueng/received_events","type":"User","site_admin":false},"committer":{"login":"julianoes","id":1419688,"node_id":"MDQ6VXNlcjE0MTk2ODg=","avatar_url":"https://avatars.githubusercontent.com/u/1419688?v=4","gravatar_id":"","url":"https://api.github.com/users/julianoes","html_url":"https://github.com/julianoes","followers_url":"https://api.github.com/users/julianoes/followers","following_url":"https://api.github.com/users/julianoes/following{/other_user}","gists_url":"https://api.github.com/users/julianoes/gists{/gist_id}","starred_url":"https://api.github.com/users/julianoes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/julianoes/subscriptions","organizations_url":"https://api.github.com/users/julianoes/orgs","repos_url":"https://api.github.com/users/julianoes/repos","events_url":"https://api.github.com/users/julianoes/events{/privacy}","received_events_url":"https://api.github.com/users/julianoes/received_events","type":"User","site_admin":false},"parents":[{"sha":"b6ba7b655aff9a273a4643b2c3e3a76d1098b8b8","url":"https://api.github.com/repos/PX4/PX4-Autopilot/commits/b6ba7b655aff9a273a4643b2c3e3a76d1098b8b8","html_url":"https://github.com/PX4/PX4-Autopilot/commit/b6ba7b655aff9a273a4643b2c3e3a76d1098b8b8"}],"stats":{"total":152,"additions":118,"deletions":34},"files":[{"sha":"a51e57cb1cec37d057bb14c5fd83bef67252046a","filename":"platforms/posix/src/lockstep_scheduler/CMakeLists.txt","status":"modified","additions":2,"deletions":0,"changes":2,"blob_url":"https://github.com/PX4/PX4-Autopilot/blob/828e31d3a914166c8bb3897c6191f4203f151b7a/platforms%2Fposix%2Fsrc%2Flockstep_scheduler%2FCMakeLists.txt","raw_url":"https://github.com/PX4/PX4-Autopilot/raw/828e31d3a914166c8bb3897c6191f4203f151b7a/platforms%2Fposix%2Fsrc%2Flockstep_scheduler%2FCMakeLists.txt","contents_url":"https://api.github.com/repos/PX4/PX4-Autopilot/contents/platforms%2Fposix%2Fsrc%2Flockstep_scheduler%2FCMakeLists.txt?ref=828e31d3a914166c8bb3897c6191f4203f151b7a","patch":"@@ -6,6 +6,8 @@ if(NOT PROJECT_NAME STREQUAL \"px4\")\n \n     set (CMAKE_CXX_STANDARD 11)\n \n+\tadd_definitions(-DUNIT_TESTS)\n+\n     add_library(lockstep_scheduler\n         src/lockstep_scheduler.cpp\n     )"},{"sha":"69a0ee34ed189f0b8e46e4155dc4587eb8efb4d2","filename":"platforms/posix/src/lockstep_scheduler/include/lockstep_scheduler/lockstep_scheduler.h","status":"modified","additions":19,"deletions":2,"changes":21,"blob_url":"https://github.com/PX4/PX4-Autopilot/blob/828e31d3a914166c8bb3897c6191f4203f151b7a/platforms%2Fposix%2Fsrc%2Flockstep_scheduler%2Finclude%2Flockstep_scheduler%2Flockstep_scheduler.h","raw_url":"https://github.com/PX4/PX4-Autopilot/raw/828e31d3a914166c8bb3897c6191f4203f151b7a/platforms%2Fposix%2Fsrc%2Flockstep_scheduler%2Finclude%2Flockstep_scheduler%2Flockstep_scheduler.h","contents_url":"https://api.github.com/repos/PX4/PX4-Autopilot/contents/platforms%2Fposix%2Fsrc%2Flockstep_scheduler%2Finclude%2Flockstep_scheduler%2Flockstep_scheduler.h?ref=828e31d3a914166c8bb3897c6191f4203f151b7a","patch":"@@ -10,6 +10,8 @@\n class LockstepScheduler\n {\n public:\n+\t~LockstepScheduler();\n+\n \tvoid set_absolute_time(uint64_t time_us);\n \tinline uint64_t get_absolute_time() const { return time_us_; }\n \tint cond_timedwait(pthread_cond_t *cond, pthread_mutex_t *lock, uint64_t time_us);\n@@ -19,13 +21,28 @@ class LockstepScheduler\n \tstd::atomic<uint64_t> time_us_{0};\n \n \tstruct TimedWait {\n+\t\t~TimedWait()\n+\t\t{\n+\t\t\t// If a thread quickly exits after a cond_timedwait(), the\n+\t\t\t// thread_local object can still be in the linked list. In that case\n+\t\t\t// we need to wait until it's removed.\n+\t\t\twhile (!removed) {\n+#ifndef UNIT_TESTS // unit tests don't define system_usleep and execute faster w/o sleeping here\n+\t\t\t\tsystem_sleep(5000);\n+#endif\n+\t\t\t}\n+\t\t}\n+\n \t\tpthread_cond_t *passed_cond{nullptr};\n \t\tpthread_mutex_t *passed_lock{nullptr};\n \t\tuint64_t time_us{0};\n \t\tbool timeout{false};\n \t\tstd::atomic<bool> done{false};\n+\t\tstd::atomic<bool> removed{true};\n+\n+\t\tTimedWait *next{nullptr}; ///< linked list\n \t};\n-\tstd::vector<std::shared_ptr<TimedWait>> timed_waits_{};\n-\tstd::mutex timed_waits_mutex_{};\n+\tTimedWait *timed_waits_{nullptr}; ///< head of linked list\n+\tstd::mutex timed_waits_mutex_;\n \tstd::atomic<bool> setting_time_{false}; ///< true if set_absolute_time() is currently being executed\n };"},{"sha":"08b76f0272eba41fa1b6c59f2addbbcac4fb78c6","filename":"platforms/posix/src/lockstep_scheduler/src/lockstep_scheduler.cpp","status":"modified","additions":51,"deletions":22,"changes":73,"blob_url":"https://github.com/PX4/PX4-Autopilot/blob/828e31d3a914166c8bb3897c6191f4203f151b7a/platforms%2Fposix%2Fsrc%2Flockstep_scheduler%2Fsrc%2Flockstep_scheduler.cpp","raw_url":"https://github.com/PX4/PX4-Autopilot/raw/828e31d3a914166c8bb3897c6191f4203f151b7a/platforms%2Fposix%2Fsrc%2Flockstep_scheduler%2Fsrc%2Flockstep_scheduler.cpp","contents_url":"https://api.github.com/repos/PX4/PX4-Autopilot/contents/platforms%2Fposix%2Fsrc%2Flockstep_scheduler%2Fsrc%2Flockstep_scheduler.cpp?ref=828e31d3a914166c8bb3897c6191f4203f151b7a","patch":"@@ -1,5 +1,16 @@\n #include \"lockstep_scheduler/lockstep_scheduler.h\"\n \n+LockstepScheduler::~LockstepScheduler()\n+{\n+\t// cleanup the linked list\n+\tstd::unique_lock<std::mutex> lock_timed_waits(timed_waits_mutex_);\n+\n+\twhile (timed_waits_) {\n+\t\tTimedWait *tmp = timed_waits_;\n+\t\ttimed_waits_ = timed_waits_->next;\n+\t\ttmp->removed = true;\n+\t}\n+}\n \n void LockstepScheduler::set_absolute_time(uint64_t time_us)\n {\n@@ -9,29 +20,38 @@ void LockstepScheduler::set_absolute_time(uint64_t time_us)\n \t\tstd::unique_lock<std::mutex> lock_timed_waits(timed_waits_mutex_);\n \t\tsetting_time_ = true;\n \n-\t\tauto it = std::begin(timed_waits_);\n-\n-\t\twhile (it != std::end(timed_waits_)) {\n-\n-\t\t\tstd::shared_ptr<TimedWait> temp_timed_wait = *it;\n+\t\tTimedWait *timed_wait = timed_waits_;\n+\t\tTimedWait *timed_wait_prev = nullptr;\n \n+\t\twhile (timed_wait) {\n \t\t\t// Clean up the ones that are already done from last iteration.\n-\t\t\tif (temp_timed_wait->done) {\n-\t\t\t\tit = timed_waits_.erase(it);\n+\t\t\tif (timed_wait->done) {\n+\t\t\t\t// Erase from the linked list\n+\t\t\t\tif (timed_wait_prev) {\n+\t\t\t\t\ttimed_wait_prev->next = timed_wait->next;\n+\n+\t\t\t\t} else {\n+\t\t\t\t\ttimed_waits_ = timed_wait->next;\n+\t\t\t\t}\n+\n+\t\t\t\tTimedWait *tmp = timed_wait;\n+\t\t\t\ttimed_wait = timed_wait->next;\n+\t\t\t\ttmp->removed = true;\n \t\t\t\tcontinue;\n \t\t\t}\n \n-\t\t\tif (temp_timed_wait->time_us <= time_us &&\n-\t\t\t    !temp_timed_wait->timeout) {\n+\t\t\tif (timed_wait->time_us <= time_us &&\n+\t\t\t    !timed_wait->timeout) {\n \t\t\t\t// We are abusing the condition here to signal that the time\n \t\t\t\t// has passed.\n-\t\t\t\tpthread_mutex_lock(temp_timed_wait->passed_lock);\n-\t\t\t\ttemp_timed_wait->timeout = true;\n-\t\t\t\tpthread_cond_broadcast(temp_timed_wait->passed_cond);\n-\t\t\t\tpthread_mutex_unlock(temp_timed_wait->passed_lock);\n+\t\t\t\tpthread_mutex_lock(timed_wait->passed_lock);\n+\t\t\t\ttimed_wait->timeout = true;\n+\t\t\t\tpthread_cond_broadcast(timed_wait->passed_cond);\n+\t\t\t\tpthread_mutex_unlock(timed_wait->passed_lock);\n \t\t\t}\n \n-\t\t\t++it;\n+\t\t\ttimed_wait_prev = timed_wait;\n+\t\t\ttimed_wait = timed_wait->next;\n \t\t}\n \n \t\tsetting_time_ = false;\n@@ -40,7 +60,9 @@ void LockstepScheduler::set_absolute_time(uint64_t time_us)\n \n int LockstepScheduler::cond_timedwait(pthread_cond_t *cond, pthread_mutex_t *lock, uint64_t time_us)\n {\n-\tstd::shared_ptr<TimedWait> new_timed_wait;\n+\t// A TimedWait object might still be in timed_waits_ after we return, so its lifetime needs to be\n+\t// longer. And using thread_local is more efficient than malloc.\n+\tstatic thread_local TimedWait timed_wait;\n \t{\n \t\tstd::lock_guard<std::mutex> lock_timed_waits(timed_waits_mutex_);\n \n@@ -49,22 +71,29 @@ int LockstepScheduler::cond_timedwait(pthread_cond_t *cond, pthread_mutex_t *loc\n \t\t\treturn ETIMEDOUT;\n \t\t}\n \n-\t\tnew_timed_wait = std::make_shared<TimedWait>();\n-\t\tnew_timed_wait->time_us = time_us;\n-\t\tnew_timed_wait->passed_cond = cond;\n-\t\tnew_timed_wait->passed_lock = lock;\n-\t\ttimed_waits_.push_back(new_timed_wait);\n+\t\ttimed_wait.time_us = time_us;\n+\t\ttimed_wait.passed_cond = cond;\n+\t\ttimed_wait.passed_lock = lock;\n+\t\ttimed_wait.timeout = false;\n+\t\ttimed_wait.done = false;\n+\n+\t\t// Add to linked list if not removed yet (otherwise just re-use the object)\n+\t\tif (timed_wait.removed) {\n+\t\t\ttimed_wait.removed = false;\n+\t\t\ttimed_wait.next = timed_waits_;\n+\t\t\ttimed_waits_ = &timed_wait;\n+\t\t}\n \t}\n \n \tint result = pthread_cond_wait(cond, lock);\n \n-\tconst bool timeout = new_timed_wait->timeout;\n+\tconst bool timeout = timed_wait.timeout;\n \n \tif (result == 0 && timeout) {\n \t\tresult = ETIMEDOUT;\n \t}\n \n-\tnew_timed_wait->done = true;\n+\ttimed_wait.done = true;\n \n \tif (!timeout && setting_time_) {\n \t\t// This is where it gets tricky: the timeout has not been triggered yet,"},{"sha":"e486710bf24bd8e035e16670daa03c2bad1311a6","filename":"platforms/posix/src/lockstep_scheduler/test/src/lockstep_scheduler_test.cpp","status":"modified","additions":46,"deletions":10,"changes":56,"blob_url":"https://github.com/PX4/PX4-Autopilot/blob/828e31d3a914166c8bb3897c6191f4203f151b7a/platforms%2Fposix%2Fsrc%2Flockstep_scheduler%2Ftest%2Fsrc%2Flockstep_scheduler_test.cpp","raw_url":"https://github.com/PX4/PX4-Autopilot/raw/828e31d3a914166c8bb3897c6191f4203f151b7a/platforms%2Fposix%2Fsrc%2Flockstep_scheduler%2Ftest%2Fsrc%2Flockstep_scheduler_test.cpp","contents_url":"https://api.github.com/repos/PX4/PX4-Autopilot/contents/platforms%2Fposix%2Fsrc%2Flockstep_scheduler%2Ftest%2Fsrc%2Flockstep_scheduler_test.cpp?ref=828e31d3a914166c8bb3897c6191f4203f151b7a","patch":"@@ -4,7 +4,43 @@\n #include <atomic>\n #include <random>\n #include <iostream>\n+#include <functional>\n+#include <chrono>\n \n+class TestThread\n+{\n+public:\n+\tTestThread(const std::function<void()> &f)\n+\t\t: _f(f)\n+\t{\n+\t\t_thread = std::thread(std::bind(&TestThread::execute, this));\n+\t}\n+\n+\tvoid join(LockstepScheduler &ls)\n+\t{\n+\t\t// The unit-tests do not reflect the real usage, where\n+\t\t// set_absolute_time() is called regularly and can do some\n+\t\t// cleanup tasks. We simulate that here by waiting until\n+\t\t// the given task returns (which is expected to happen quickly)\n+\t\t// and then call set_absolute_time(), which can do the cleanup,\n+\t\t// and _thread can then exit as well.\n+\t\twhile (!_done) {\n+\t\t\tstd::this_thread::yield(); // usleep is too slow here\n+\t\t}\n+\n+\t\tls.set_absolute_time(ls.get_absolute_time());\n+\t\t_thread.join();\n+\t}\n+private:\n+\tvoid execute()\n+\t{\n+\t\t_f();\n+\t\t_done = true;\n+\t}\n+\tstd::function<void()> _f;\n+\tstd::atomic<bool> _done{false};\n+\tstd::thread _thread;\n+};\n \n constexpr uint64_t some_time_us = 12345678;\n \n@@ -33,7 +69,7 @@ void test_condition_timing_out()\n \n \t// Use a thread to wait for condition while we already have the lock.\n \t// This ensures the synchronization happens in the right order.\n-\tstd::thread thread([&ls, &cond, &lock, &should_have_timed_out]() {\n+\tTestThread thread([&ls, &cond, &lock, &should_have_timed_out]() {\n \t\tassert(ls.cond_timedwait(&cond, &lock, some_time_us + 1000) == ETIMEDOUT);\n \t\tassert(should_have_timed_out);\n \t\t// It should be re-locked afterwards, so we should be able to unlock it.\n@@ -44,7 +80,7 @@ void test_condition_timing_out()\n \tshould_have_timed_out = true;\n \tls.set_absolute_time(some_time_us + 1500);\n \n-\tthread.join();\n+\tthread.join(ls);\n \n \tpthread_mutex_destroy(&lock);\n \tpthread_cond_destroy(&cond);\n@@ -67,7 +103,7 @@ void test_locked_semaphore_getting_unlocked()\n \tpthread_mutex_lock(&lock);\n \t// Use a thread to wait for condition while we already have the lock.\n \t// This ensures the synchronization happens in the right order.\n-\tstd::thread thread([&ls, &cond, &lock]() {\n+\tTestThread thread([&ls, &cond, &lock]() {\n \n \t\tls.set_absolute_time(some_time_us + 500);\n \t\tassert(ls.cond_timedwait(&cond, &lock, some_time_us + 1000) == 0);\n@@ -79,7 +115,7 @@ void test_locked_semaphore_getting_unlocked()\n \tpthread_cond_broadcast(&cond);\n \tpthread_mutex_unlock(&lock);\n \n-\tthread.join();\n+\tthread.join(ls);\n \n \tpthread_mutex_destroy(&lock);\n \tpthread_cond_destroy(&cond);\n@@ -107,7 +143,7 @@ class TestCase\n \tvoid run()\n \t{\n \t\tpthread_mutex_lock(&lock_);\n-\t\tthread_ = std::make_shared<std::thread>([this]() {\n+\t\tthread_ = std::make_shared<TestThread>([this]() {\n \t\t\tresult_ = ls_.cond_timedwait(&cond_, &lock_, timeout_);\n \t\t\tpthread_mutex_unlock(&lock_);\n \t\t});\n@@ -131,13 +167,13 @@ class TestCase\n \t\t\tpthread_mutex_unlock(&lock_);\n \t\t\tis_done_ = true;\n \t\t\t// We can be sure that this triggers.\n-\t\t\tthread_->join();\n+\t\t\tthread_->join(ls_);\n \t\t\tassert(result_ == 0);\n \t\t}\n \n \t\telse if (timeout_reached) {\n \t\t\tis_done_ = true;\n-\t\t\tthread_->join();\n+\t\t\tthread_->join(ls_);\n \t\t\tassert(result_ == ETIMEDOUT);\n \t\t}\n \t}\n@@ -151,7 +187,7 @@ class TestCase\n \tLockstepScheduler &ls_;\n \tstd::atomic<bool> is_done_{false};\n \tstd::atomic<int> result_ {INITIAL_RESULT};\n-\tstd::shared_ptr<std::thread> thread_{};\n+\tstd::shared_ptr<TestThread> thread_{};\n };\n \n int random_number(int min, int max)\n@@ -250,7 +286,7 @@ void test_usleep()\n \n \tstd::atomic<Step> step{Step::Init};\n \n-\tstd::thread thread([&step, &ls]() {\n+\tTestThread thread([&step, &ls]() {\n \t\tstep = Step::ThreadStarted;\n \n \t\tWAIT_FOR(step == Step::BeforeUsleep);\n@@ -268,7 +304,7 @@ void test_usleep()\n \n \tassert(ls.usleep_until(some_time_us + 1000) == 0);\n \tassert(step == Step::UsleepTriggered);\n-\tthread.join();\n+\tthread.join(ls);\n }\n \n int main(int /*argc*/, char ** /*argv*/)"}]}